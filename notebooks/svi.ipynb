{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting GET to https://tiles.mapillary.com/maps/vtp/mly1_public/2/14/12917/8132/?access_token=MLY%7C5261155797330596%7C84cc24c214184c5aa4b5ce85687734ad\n",
      "Response 200 OK received in 1425ms\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Getting image coordinates from a nearby set of coordinates\n",
    "\"\"\"\n",
    "\n",
    "# Importing mapillary\n",
    "import mapillary.interface as mly\n",
    "import mapillary.utils as mlu\n",
    "# JSON import\n",
    "import json\n",
    "\n",
    "mlu.auth.set_token(\"MLY|5261155797330596|84cc24c214184c5aa4b5ce85687734ad\")\n",
    "latitude = 1.3019\n",
    "longitude = 103.8381\n",
    "\n",
    "# Get image points close to in the given coordinates\n",
    "data = mly.get_image_close_to(longitude=longitude, latitude=latitude).to_dict()\n",
    "\n",
    "# # Save the data as JSON\n",
    "# file_name = \"data/external/get_image_close_to_1.json\"\n",
    "# with open(file_name, mode=\"w\") as f:\n",
    "#     json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "for point in data[\"features\"]:\n",
    "    url = mly.image_thumbnail(image_id = point[\"properties\"][\"id\"], resolution=2048)\n",
    "    fn = f\"data/external/svi_examples/{point['properties']['id']}.jpg\"\n",
    "    print(fn)\n",
    "    try:\n",
    "        if not os.path.exists(fn):\n",
    "            r = requests.get(url)\n",
    "            with open(fn, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "    except Exception as e:\n",
    "        print('Exception in download_url():', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "def create_cityscapes_label_colormap():\n",
    "    \"\"\"Creates a label colormap used in CITYSCAPES segmentation benchmark.\n",
    "    Returns:\n",
    "        A colormap for visualizing segmentation results.\n",
    "    \"\"\"\n",
    "    # a label and all meta information\n",
    "    Label = namedtuple( 'Label' , [\n",
    "\n",
    "        'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n",
    "                        # We use them to uniquely name a class\n",
    "\n",
    "        'id'          , # An integer ID that is associated with this label.\n",
    "                        # The IDs are used to represent the label in ground truth images\n",
    "                        # An ID of -1 means that this label does not have an ID and thus\n",
    "                        # is ignored when creating ground truth images (e.g. license plate).\n",
    "                        # Do not modify these IDs, since exactly these IDs are expected by the\n",
    "                        # evaluation server.\n",
    "\n",
    "        'trainId'     , # Feel free to modify these IDs as suitable for your method. Then create\n",
    "                        # ground truth images with train IDs, using the tools provided in the\n",
    "                        # 'preparation' folder. However, make sure to validate or submit results\n",
    "                        # to our evaluation server using the regular IDs above!\n",
    "                        # For trainIds, multiple labels might have the same ID. Then, these labels\n",
    "                        # are mapped to the same class in the ground truth images. For the inverse\n",
    "                        # mapping, we use the label that is defined first in the list below.\n",
    "                        # For example, mapping all void-type classes to the same ID in training,\n",
    "                        # might make sense for some approaches.\n",
    "                        # Max value is 255!\n",
    "\n",
    "        'category'    , # The name of the category that this label belongs to\n",
    "\n",
    "        'categoryId'  , # The ID of this category. Used to create ground truth images\n",
    "                        # on category level.\n",
    "\n",
    "        'hasInstances', # Whether this label distinguishes between single instances or not\n",
    "\n",
    "        'ignoreInEval', # Whether pixels having this class as ground truth label are ignored\n",
    "                        # during evaluations or not\n",
    "\n",
    "        'color'       , # The color of this label\n",
    "        ] )\n",
    "\n",
    "    labels = [\n",
    "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
    "    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
    "    Label(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
    "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
    "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
    "    Label(  'parking'              ,  9 ,      255 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
    "    Label(  'rail track'           , 10 ,      255 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
    "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
    "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
    "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    Label(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
    "    Label(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
    "    Label(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
    "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
    "    Label(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
    "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
    "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
    "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
    "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
    "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
    "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
    "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
    "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
    "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
    "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
    "    Label(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
    "    Label(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
    "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
    "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
    "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
    "    Label(  'license plate'        , -1 ,       -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
    "    ]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/koichiito/opt/anaconda3/envs/vis_review/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/koichiito/opt/anaconda3/envs/vis_review/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image, ImageEnhance\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "\n",
    "def segment_image(image_path, model, device):\n",
    "    img = Image.open(image_path)\n",
    "    transform = T.Compose([\n",
    "        T.Resize((1024, 2048)),\n",
    "        T.ToTensor()#,\n",
    "        # T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    input_img = transform(img).unsqueeze(0).to(device)\n",
    "    output = model(input_img)['out'][0].argmax(0).byte().cpu().numpy()\n",
    "    return output.astype('uint8')\n",
    "\n",
    "def overlay_images(image1, image2, alpha):\n",
    "    blended_img = Image.blend(image1, image2, alpha)\n",
    "    return blended_img\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "model = models.segmentation.deeplabv3_resnet50(pretrained=True, progress=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "image_path = 'data/external/svi_examples/365237318245343.jpg'\n",
    "labels = create_cityscapes_label_colormap()\n",
    "original_img = Image.open(image_path).resize((2048, 1024))\n",
    "segmented_img = segment_image(image_path, model, device)\n",
    "# Finally we visualize the prediction\n",
    "color_seg = np.zeros((segmented_img.shape[0], segmented_img.shape[1], 3), dtype=np.uint8)\n",
    "unique_id_array = np.unique(segmented_img)\n",
    "for unique_id in unique_id_array:\n",
    "    label = list(filter(lambda label: label.trainId == unique_id, labels))[0]\n",
    "                        # give color to color_seg\n",
    "    color_seg[segmented_img == unique_id, :] = label.color\n",
    "# segmented_img_color.putpalette([\n",
    "#     128, 64, 128,  # Road\n",
    "#     70, 70, 70,  # Sidewalk\n",
    "#     153, 153, 153,  # Building\n",
    "#     107, 142, 35,  # Vegetation\n",
    "#     70, 130, 180,  # Sky\n",
    "#     220, 20, 60,  # Person\n",
    "#     255, 0, 0,  # Car\n",
    "#     0, 0, 142,  # Truck\n",
    "#     # Add more colors if necessary\n",
    "# ])\n",
    "color_seg = Image.fromarray(color_seg)\n",
    "segmented_img_color = color_seg.resize((2048, 1024), Image.NEAREST)\n",
    "\n",
    "alpha = 0.5\n",
    "overlay = overlay_images(original_img, segmented_img_color, alpha)\n",
    "overlay.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image, ImageEnhance\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "\n",
    "def segment_image(image_path, model, device):\n",
    "    img = Image.open(image_path)\n",
    "    transform = T.Compose([\n",
    "        T.Resize((1024, 2048)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    input_img = transform(img).unsqueeze(0).to(device)\n",
    "    output = model(input_img)['out'][0].argmax(0).byte().cpu().numpy()\n",
    "    return output.astype('uint8')\n",
    "\n",
    "def label_to_color_image(label_image):\n",
    "    labels = create_cityscapes_label_colormap()\n",
    "    color_image = np.zeros((label_image.shape[0], label_image.shape[1], 3), dtype=np.uint8)\n",
    "    for label in labels:\n",
    "        color_image[label_image == label.id] = label.color\n",
    "    return color_image\n",
    "\n",
    "def overlay_images(image1, image2, alpha):\n",
    "    blended_img = Image.blend(image1, image2, alpha)\n",
    "    return blended_img\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "model = models.segmentation.deeplabv3_resnet50(pretrained=True, progress=True)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "image_path = 'data/external/svi_examples/365237318245343.jpg'\n",
    "\n",
    "original_img = Image.open(image_path).resize((2048, 1024))\n",
    "segmented_img = segment_image(image_path, model, device)\n",
    "segmented_img_color = Image.fromarray(label_to_color_image(segmented_img), 'RGB')\n",
    "segmented_img_color = segmented_img_color.resize((2048, 1024), Image.NEAREST)\n",
    "# segmented_img_color = segmented_img_color.convert('RGBA')\n",
    "\n",
    "alpha = 0.5\n",
    "overlay = overlay_images(original_img, segmented_img_color, alpha)\n",
    "overlay.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACAAAAAQACAIAAAD6ZS4TAAAonElEQVR4nO3dO3bbSAIFUEhH6/Ba7FyBc6eMtAquQskwVe6Aub2AWYVWMgH7sDkUCRaA+te9p5Pu1qcA8Yf3qgrTBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACQ2FPpAQAAAAAAcNf7bnf5r2+HQ6mR0BwFAAAAAABAda5y/ytqAEIoAAAAAACA9c45tUg6lvno/4rTzgwFAAAAAACwmPnpKSyK/i854dykAAAAAAAAFjA//Z6NiyFWp/9bfil9UwAAAAAAwHC+Zs0h8bH56TM2LonYmP4H/hZGowAAAAAAgIE8DJrPIXKUSPrrj+1SyLmaOQMRT3Xf55mlFAAAAAAAMIS4gf4KvWbTG/dEiv536fU8s8Jz6QEAAAAAAMkVT/8rGUN0Sw/q6utTnJMuzzPrKAAAAAAAoGfvu109iXA9Iykow0lwnjmxBRAAAAAAdKvOILibPWrqPL1n3ZxnVrMCAAAAAAD6VG08Xe3AOuM8owAAAAAAAFisiXi9iUGSjgIAAAAAADpUefJb+fCgDwoAAAAAAOhNE/F6E4PsgPM8MgUAAAAAAFCGbBqSUgAAAAAAQFek6lzxkBiWAgAAAAAAKEY2nYfzPCYFAAAAAAD0o8Wct8UxQxMUAAAAAAAA/VO0DEgBAAAAAACdaDfhbXfkUDMFAAAAAABQng4gAyd5NAoAAAAAAOhBB9nu+27XwVFUzhkeigIAAAAAAKiIhDo1Z3gcCgAAAAAAaF5nkW5nhwOlKAAAAAAAgOroAJJyegehAAAAAACAtvUa5vZ6XJVwekegAAAAAAAAKiWkhi0UAAAAAAAAI9KvdE8BAAAAAADUS0gNqykAAAAAAICq6QDScW77pgAAAAAAAIAOKQAAAAAAgNqZqJ6Oc9sxBQAAAAAAAHRIAQAAAAAANMBE9XSc214pAAAAAAAAoEMKAAAAAAAA6JACAAAAAABog51q0nFuu6QAAAAAAACADikAAAAAAACgQwoAAAAAAIDF3g6H0kOIzC5A/VEAAAAAAADNEFJDOAUAAAAAAAB0SAEAAAAAAMA0WWDRHQUAAAAAANASITUEUgAAAAAAACymh6B+CgAAAAAAAOiQAgAAAAAA2vZ2OJQeQm5m36fj3PZEAQAAAAAAAB1SAAAAAAAA7TFRHR5SAAAAAAAAQIcUAAAAAABAkwouAuh7/UHfRzcUBQAAAAAAAHRIAQAAAAAAAB1SAAAAAABA894Oh9JDKMNmNYk4sX1QAAAAAAAALCAcpxUKAAAAAACgYeL4RJzYDigAAAAAAAC4QQfQOgUAAAAAANA2OXU6zm3TFAAAAAAAANAhBQAAAAAAAHRIAQAAAAAAwF12AWqXAgAAAAAAADqkAAAAAACAHrwdDqWHUJJZ6vCVAgAAAAAAADqkAAAAAAAAgA4pAAAAAAAAmGOHpUYpAAAAAAAAmDP4HSbapQAAAAAAgE5IaYFLCgAAAAAAoHnKD/hKAQAAAAAAsICygVYoAAAAAACAtknkk3J626UAAAAAAABYZpxMfJwj7ZICAAAAAABomIQ6Hee2dQoAAAAAAOjHaIltwePt/lR3f4AjeCk9AAAAAACANSTUiTix3XgqPQAAAAAAILL33a70ENKqJ6Hu6VTXc1aJRQEAAAAAAB3qKZg+qzahXnS2lx7F1Q9fdxJujrDa80ksCgAAAAAA6FPZDuAULkcZQxM59fyRNnEI9EcBAAAAAADdytwBhMfcIQMTmsNGCgAAAAAA6Fl4B3AZuK/7rtXOv07oDwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA0Kmn0gMAGN37bnf1X94OhyIjAQAAAKAnCgCAMr7m/l9pAgAAAABY7bn0AABGFJL+h38ZAAAAAHylAADIbVGsrwMAAAAAYB1bAAFschnQh+zYsy7QtxcQAAAAAEspAABWuhflz4T1W6bz6wAAAAAAWEQBALDGfJT/NayPspOPDgAAAACAcO4BALDM+273MM2/+gL7+AMAAACQnwIAYIHwKP/8lRHTf0UCAAAAAOFsAQQQqob83S5AAAAAAASyAgAgSA3p/1TNMAAAAAConwIAAAAAAAA6pAAAeKyqefdVDQYAAACAaikAAB4QuAMAAADQIgUAAAAAAAB0SAEAMKfO6f91jgoAAACAqigAAJqkAwAAAABgngIA4C4hOwAAAADtUgAAtEo/AQAAAMAMBQDAbeJ1AAAAAJqmAADI7e1wKD0EAAAAAPqnAADI6pT+6wAAAAAASE0BAHBDK/v/tDJOAAAAAPJTAADkcznx3yIAAAAAAJJ6KT0AgOqYVj/dOgkaCwAAAIC2PJUeAEB1EhUANwP07b8rbi7/cDxqAAAAAIBWWAEA8H/GnP4/5lEDAAAA9M09AACKqSR2Dx+G6f8AAAAADVEAAGRSSdx/pc5RAQAAALCdAgBgXIvSf9P/AQAAANqiAAAYlLn/AAAAAH1TAAD8a5xMfJwjBQAAABiWAgAgn8vYvWAEL/0HAAAAGIECAKBhK/blX5f+uwEAAAAAQHMUAAD/yDMv3ux7AAAAAPJQAADk9r7bFawBVszlN/0fAAAAoEUKAIBW5cnlpf8AAAAAjVIAAEzTYDvzhGf60n8AAACAdr2UHgAAa6SO5kX/AAAAAK2zAgBgrOn/JzP5/tvhIP0HAAAA6IACAGBQN1N+0T8AAABAN55KDwCgsBan/0eM6c+HL/oHAAAA6IwCABhai+n/JKwHAAAAIIAtgAAaI/0HAAAAIIQCABhXi9P/pf8AAAAABFIAAIOS/gMAAADQNwUAQBuk/wAAAAAsogAARtTc9H/pPwAAAABLKQAAaif9BwAAAGCFp9IDAMitoen/on8AAAAAVnspPQAAbhD9AwAAALCRLYAAqiP9BwAAAGA7KwAAKiL6BwAAACAW9wAARlTbbQDk/gAAAABEZwUAQEmifwAAAAAScQ8AgGKk/wAAAACkYwsgYFBldwES/QMAAACQmhUAwKAKRvDSfwAAAAAysAIA4LZESwSk/wAAAADkoQAAeCBiEyD9BwAAACAbBQBAkO01gPQfAAAAgJwUAADLrGgCRP8AAAAA5KcAAFgpsAmQ/gMAAABQhAIAYKvLJkDcDwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPV43+1KDwEASOi59AAAAACAAk7pvw4AADqmAAAAAAAAgA4pAAAAAGBEb4dD6SEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAApPO+273vdqVHAQAAANCtl9IDAGA4cn8AAACADJ5LDwCAsVyl/8oAAAAAgEQUAADkI+4HAAAAyEYBAEAm0n8AAACAnBQAAOQg/QcAAADITAEAQHLSfwAAAID8FAAApCX9BwAAACjipfQAAAAAAGjb/vs+yY/9m+THAoxDAQAAAADAYolC/5lfoQ8AWOqp9AAA6FnI/j9vh0OGkQAAABFlSP/v/mo1AEAwKwAAAAAAgAbcnGRmVhnMUAAAAAAA0Iz9971FAGO6t8T89N/T1QAR17t46JKfLYAASMX+PwAA0KWC+//8MwAp6nhCLjCnqNeYqR/nHsbkoQAAIBUFAAAAdKl4ATAJTwdz/DhO0/T55/ei71p9vZn5Ee7BTFK2AAIgicDZGQAAADDjlP6v8L7bLeoASjVbdrUiKSsAAIgvPP23AgAAANpSw/T/E5npCC7T/6UrAM5CLjxreGB7SJOCAgCAmJZO/FcAQE82XjW54AGAJtSQk575/NC3q7n/qwuAk5nLz3oe1R7SRPdcegAA9EP6DyPbftW0/74//RNhNADAAESlfVu9889SVX3+rGow9EEBAEAc0n8AACCP/V97pvfv9ddr3B9476LVY4m+uQkwALmJ/oF5boMGANzjQ8JQTh1AtqUA0CUFAABZSf8BAIAVRP/DOi8FWLruHJgUAABEEfg5TPoPAAAsIvcng/1fd6KiWwoAADKR/kPHXC9Ny0+COAMAZnij7Nt5V5/oG/3Pe9/tXJkyGgUAAFuFTP/3GQvo0pbm4+p7i8ccgcdSfJwAdM97Td+uNvQv1QR8ZREAvVIAALCJ9B8YU/Trw9MPzBZ5rB7/zW+U1ACwnXeTEczczvf4cXzYAbwdDklvA6ADoEsKAAAANklxmbT/vq85BUh3ZZi6Bkg08vOPrfmvBkAUSd73vX30bib3//plZZcCnB6NaoATMz/68FR6AAA0zL1/gSldplzrpUWeC8IUh5/tUraev92iQ65n2AA1i78GzstvpwJD/3vu1QBRVgCEX6LmbwLqeUbMH3s94+QhKwAASM59loBu5MvQo66ByHztGnEdQ86Rf/1drmwBkvIy25mNif/Xn1b8rgDTxaN0tDUBD4/XAtCGKAAAACBIozF6qevV1eOv5wL7ciQubgGmsJfokB1UvKj2J276f/6ZqTuA+WFf/vbLB209n1VqoAmony2AAFhp0dJLKwCgY4NsAVT2Sm/d2ajt6nTmKGobagq1PaQBAlkgxUMp0v+TrwVAlC2Avv34uX0kU9c7Yq0+tHoOgTMrAAAA4IHi8fSKHYGKj/mrCoeUk9voAS0a/KWbEOnS/6majYCmW4f5+ut12A2CZkTcCpJYFAAArBFl2gVAtSq8igu/mqpw8NxjlyGgTt5KCJQ0/U9kxfT/my6P/dRSbHzi9PRJ4N6p6OkYG2ILIAAWW5f+2wUIetXBFkB9xBz7v/s+DgTXxkApge8jXqY4yZP+X60A2DgXLVb6P+P11+uKj2RVPa3yf6Ss6vD7YwUAAAA1WrHpzepflOG3ZNDNgZDtwQ8Aq7U493+aps8/v1N3AEvPzH//89/a3vfzTyvx4ScpKwAAWGPptAvT/6FvHSwCmJoN0K/OUqNHwZmrX6Cg8DcRL1ZkKwAuVwDE2oo2wzqApSq51cFZkY+UXlgSeS49AACaJNAH+rP/uz/9U3ogQe6NtpXxc5M/H9AKfTP5RbwR3eef37F+VK98JumJLYAAWOntcHArYKBLpwuemqON1i/J3K7gptb/rMBolm7ZMf/K7zWQGSkuPDPsBbTI8eNY2yKA/GwElIgVAACsZx0AkFTZjLjdy4/KR/5Pv9LOYos8nA2gRQu2DHr0lfvvuuGW5LwBQLppZ59/fle1FKDR2yrE5XUgBQUAAJvoAICO1ZnJ1jmqQNd3LGj5WAD6s+JlOSS4X9QTiP+4lCGgr6oGqKoD8DmtGwoAAADqVTwFaPTKp85h3xxVnUPNzEkAWjfzfr3irbz4uz8npz6m4J8jZy5fTw1QVQdQhFeA6BQAAGxlEQCQVPFrAOFsarYDAujAzffr1W/ixd/9ufwTnJuA7pdonGqASpqAGpT6hNb3wyw/BQAAEegAYHDprg0qiYZrGMNJPSNZ6uHIK/lb5zfmUQPVqudFSQJY0PzJzzNFfeQU3iKAyStAVE+lBwBADx7el0lDAHSg+HXI0lCm+IDP1m0qHX8c9aknaAO4tO5F+PpGLzFeyb1O5je3odPf/TRGAfDtx8+Cv32aptdfr2UHcKnUpzJP/1heSg8AAADasP9bctn7mgy96IAvh7Hxu2o4invuHd3D9ASgZuveQfbf46/lSvEzmXHv737+K5icDs1RAAAAQKhSkXq72UeUkc//kId/kZlvXxNvhR1Ru38ygJONb3k1d7cwr/j0/2majh/HehYBFPkA7KNURAoAALZ6uP8PQE8eXgLFjZvnf2DI93YfwWw8P9HGAUAaFgHkEbJ2Ldv0/5FvAFCnET5SdkwBAAAAy9y8BArJJr5+zZbZ65Vrd+QAnKyI/BKF9TqApAS7hDg/Bz1gmqMAAGAT0/+BMV1mIpXPQM88Y0tAA9CTdR1AkqFQ2iC7/9ew/0/lWrlRE2cKAAAAWKOhpHvrPs6PtuBv6FQA0ChvN4kEfkLIlv6fNr43z6wVlgU0QQEAAAD9uwpN1m1hFPKTAejMw3eQRV9GW7zLE+j0UIn1xPfAi+up9AAAaFj4vIy3wyHpSAAAgNrErQFkgolc/pnuneQ8KwBO0/+ncisAqtr/53w22rLxWe9pnoIVAAAAAADEF3FSsFgwnYfndpDd/6vSaPo/bdt50tM8kefSAwAAAACgW0I9QhSPvKua/t80T/naKAAAAAAASGhjIChPLGiQ6f/S/7hWPGc9zdNRAACwUqltGQEAgOa423yLsqX/Zaf/15b+F18MEcWiZ66neVIKAAAAAACSMymYm6T/vdr/3T98Cod8DRspAAAAAADIwaTghgyy+Q+p3Xsii/6zeSk9AACaZP8fAABghcvIb/99P/N/GUHB6f/m/mfjeV3WU+kBANCepen/2+GQaCQAAAAkknoRwL30P/WEs8qj/z7uAUA9bAEEAAAAANQi3Ryybz9+Vp7+Q3S2AAIgFRP/AQAAuCnzPHe5P8OyAgCAZQIXY0r/AQAAqIH0n5FZAQBAZKJ/AACADrz+ek10G4Cc0/+l/wzOCgAAYpL+AwAAUAnpPygAAIhG+g8AAMB2Ua4upf8wTdNT6QEA0JirewAI/QEAADoWfRegkP1/Am8+N6/dAiDzHZLpmwIAAAAAALgrbgfwMN0ePP2fFABEZQsgAAAAAOAueTS066X0AAAAAACAqr3+eo2+F9BXUeb+A5cUAAAAAABASaJ/SEQBAAAAAAA8kGgRgOgfklIAAAAAAAC5if7vOX4c3XeBWBQAAAAAAEAOn39+y/0hp+fSAwAAAAAAGrBxWvrnn9+RBgKEUgAAAAAAAEFWdwDSfyhCAQAAAAAAhFrRAUj/oRQFAAAAAACwwKIOQPoPBSkAAAAAAIBlAjsA6T+UpQAAAAAAAKjI8eNYegh0QgEAAAAAACz2cBFAqen/3378LPJ7oUIKAAAAAABgjZkOwOY/UAMFAAAAAAAAdEgBAAAAAAAAHVIAAAAAAABruFctVE4BAAAAAABQkYc3WIZACgAAAAAAoBPffvwsPQSoiAIAAAAAAIjp88/v0kMApkkBAAAAAAAAXVIAAAAAAAA9sP8PXFEAAAAAAACLHT+O9/6XIB4qoQAAAAAAAJrXU+swU67AIgoAAAAAAGCZhwl15ji+p/T/RAdAFAoAAAAAAKBh/aX/J8ePoxqAjZ5KDwAAAAAAaEl4Kv3553fKgXQb/d/0+uu19BBojwIAAAAAAAi1dE56ug5gqPT/kiaAcLYAAgAAAAAaM2z6P9kaiCWsAAAAAAAAgqzLnaMvAhg5/f/KggBmWAEAAAAAACQUN6+X/l+xGoAZCgAAAAAA4LEtQbPUHopQAAAAAAAAyUXpABQJsIgCAAAAAABogPT/HrsAcY8CAAAAAADIQYIPmSkAAAAAAIBMVncAygNYQQEAAAAAAOSzIsqX/sM6CgAAAAAA4IG4u8x/+/EzPNOX/sNqCgAAAAAAoICQZF/6H+L112vpIVApBQAAAAAAUIZ8fzvpPzMUAAAAAABAMTMdgHrgIek/855KDwAAAAAAaEDc2wBc+fzz++q/SP/nif4JYQUAAAAAAFDYVdwv/Z8n/SfQS+kBAAAAAAD8Q/T/kPSfcAoAAAAAAKA80X8I6T+L2AIIAAAAAKAB0n+WUgAAAAAAANRO+s8KtgACAAAAAKiX6J/VrAAAAAAAAB4TQxfhtLOFAgAAAAAAoEbSfzZSAAAAAAAAQeTROTnbbKcAAAAAAACoi/SfKBQAAAAAAEAowTQ0RAEAAAAAAFARLQuxKAAAAAAAAKBDCgAAAAAAYAHz05NyeolIAQAAAAAALCOkhiYoAAAAAAAAqqBZIS4FAAAAAACwmKga6qcAAAAAAAAoT6dCdAoAAAAAAADokAIAAAAAAFjDjHWonAIAAAAAAFhJBwA1UwAAAAAAABSmSiGFp9IDAAAAAACad/w45vlFV0F5tt+bmgKAFBQAAAAAAEAEKbL4wFi8gxpAAUAKCgAAAAAAIJpYWfyKQLzpGkABQAoKAAAAAAAgpi1BfJQc/PhxvPdzqi0JFACkoAAAAAAAAOJbGrXnTMArrAEUAKSgAAAAAAAAkgjJ2QsG3/XUANJ/ElEAAAAAAACDqqQDUACQiAIAAAAAABhd2SZAAUAiz6UHAAAAAABQWMEIXvpPOgoAAAAAAABBPB2yBRAAAAAAwL9ybgekdSApKwAAAAAAAP6VLZSX/pOaAgAAAAAA4P9kiOal/2RgCyAAAAAAgNsSbQck/ScPBQAAAAAAwJy4NYD0n2wUAAAAAAAAj22sAeT+5KcAAAAAAABYYGkTIPoHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIBG/Q/jIqHZHxm/pAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=2048x1024>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmented_img_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/koichiito/opt/anaconda3/envs/vis_review/lib/python3.8/site-packages/transformers/models/segformer/feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/Users/koichiito/opt/anaconda3/envs/vis_review/lib/python3.8/site-packages/transformers/models/segformer/image_processing_segformer.py:102: FutureWarning: The `reduce_labels` parameter is deprecated and will be removed in a future version. Please use `do_reduce_labels` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n",
    "import requests\n",
    "\n",
    "\n",
    "def label_to_color_image(label_image):\n",
    "    labels = create_cityscapes_label_colormap()\n",
    "    color_image = np.zeros((label_image.shape[0], label_image.shape[1], 3), dtype=np.uint8)\n",
    "    for label in labels:\n",
    "        color_image[label_image == label.trainId] = label.color\n",
    "    return color_image\n",
    "\n",
    "def segment_image(image, model, feature_extractor):\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    upsampled_logits = nn.functional.interpolate(logits,\n",
    "                    size=inputs[\"pixel_values\"].shape[-2:], # (height, width)\n",
    "                    mode='bilinear',\n",
    "                    align_corners=False)\n",
    "    segmented_image = upsampled_logits.argmax(dim=1)[0].numpy()\n",
    "    return segmented_image\n",
    "\n",
    "def main():\n",
    "    model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-cityscapes-512-1024\")\n",
    "    feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-cityscapes-512-1024\")\n",
    "\n",
    "    image_path = 'data/external/svi_examples/365237318245343.jpg'\n",
    "    image = Image.open(image_path).resize((2048, 1024))\n",
    "\n",
    "    segmented_img = segment_image(image, model, feature_extractor)\n",
    "    segmented_img_color = Image.fromarray(label_to_color_image(segmented_img), 'RGB')\n",
    "    segmented_img_color = segmented_img_color.resize((2048, 1024), Image.NEAREST)\n",
    "    # segmented_img_color = segmented_img_color.convert('RGBA')\n",
    "\n",
    "    alpha = 0.5\n",
    "    overlay = overlay_images(image, segmented_img_color, alpha)\n",
    "    # save the image\n",
    "    overlay.save('data/external/svi_examples/365237318245343_segformer.jpg')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vis_review",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
